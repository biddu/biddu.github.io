---
layout: project
urltitle:  "CVAE 2019"
title: "CVAE 2019"
categories: acpr, workshop, computer vision, computer graphics, remote sensing, robotics, machine learning
permalink: /
favicon: /static/img/ico/favicon.png
bibtex: true
paper: true
acknowledgements: ""
---

<br>
<div class="row">
  <div class="col-xs-12">
    <center><h1>Computer Vision for Atmospheric Events Analysis (CVAE 2019)</h1></center>
    <center><h2>ACPR 2019 Workshop, Auckland, New Zealand</h2></center>
    <!-- <center><span style="color:#e74c3c;font-weight:400;">Time and location TBA</span></center> -->
    <center>Time TBA, <span style="color:#e74c3c;font-weight:400;">location TBA</span></center>
  </div>
</div>

<hr>

<!-- 
<div class="row" id="intro">
  <div class="col-md-12">
    <img src="{{ "/static/img/splash.png" | prepend:site.baseurl }}">
    <p> Image credit: [1, 2, 7, 12, 6, 4, 5]</p>
  </div>
</div>
-->

<br>
<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      Pattern recognition and computer vision techniques are now extensively used in the realms of remote sensing applications. These techniques assist the remote sensing analysts to understand the various atmospheric events. With the advancement of photogrammetric techniques and imaging technologies (viz. Moderate Resolution Imaging Spectroradiometer MODIS, weather satellite images, ground-based images etc.), it is now possible to effectively analyze the various events in the earth’s atmosphere. Moreover, with the advancement of computing devices and deep learning techniques, it is now increasingly easier to analyze large amounts of heterogeneous data collected from diverse sensors. Techniques from computer vision, machine learning and statistical pattern recognition have been used in a multitude of remote sensing applications: solar energy production, local weather prediction, studying atmospheric aerosols, climate change and modelling etc.
    </p>
    <p>
      This workshop aims to foster collaboration between pattern recognition and remote sensing experts. We intend to address challenging problems impacting our planet, and propose solutions using the quickly-evolving field of computer vision.
    </p>
  </div>
</div> <br>   

<div class="row">
  <div class="col-xs-12">
    <h2>Call for Papers</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      <span style="font-weight:500;">Call for papers:</span> We welcome submissions on the following topics, including but not limited to:
    </p>
    <ul>
      <li>Analysing satellite or airplane images, Moderate Resolution Imaging Spectroradiometer (MODIS) images, ground-based sky camera images etc.</li>
      <li>Multi-modal analysis of heterogeneous sensor data, multi-sensor fusion</li>
      <li>Remote sensing scene registration, segmentation, classification, analysis etc.</li>
      <li>3D reconstruction</li>
      <li>Deep learning for remote sensing</li>
      <li>Dataset papers, benchmarking, evaluation metrics etc.</li>
      <li>Pattern recognition application areas: solar energy, renewables, photovoltaic etc.</li>
    </ul>
    <p>
      <span style="font-weight:500;">Submission:</span> Paper submission portal will be available soon.
      <ul>
      <li>Submitted papers should not have been published, accepted or under review elsewhere. Non-peer reviewed media, such as Arxiv do not violate the terms.</li>
      <li>Submissions need to follow the single-blind policy and be formatted in LNCS style, with a maximum of 14 pages (including references).</li>
      <li>All the papers must be submitted using the provided templates.</li>
    </ul>      
    </p>



  </div>
</div><br>


<!-- 
The submission should be in the CVPR format.
      Reviewing will be single blind.
      Accepted extended abstracts will be made publicly available as non-archival reports, allowing future submissions to archival conferences or journals.
      We also welcome already published papers that are within the scope of the workshop (without re-formatting), including papers from the main CVPR conference.
      Please submit your paper to the following address by the deadline: <span style="color:#1a1aff;font-weight:400;"><a href="mailto:3dscenegeneration@gmail.com">3dscenegeneration@gmail.com</a></span>
      Please mention in your email if your submission has already been accepted for publication (and the name of the conference).
-->

<div class="row" id="schedule">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
  </div>
</div>


<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Paper Submission Deadline</td>
          <td>May 17 2019</td>
        </tr>
        <tr>
          <td>Notification to Authors</td>
          <td>May 31 2019</td>
        </tr>
        <tr>
          <td>Camera-Ready Deadline</td>
          <td>June 7 2019</td>
        </tr>
        <tr>
          <td>Workshop Date</td>
          <td>June 17 2019</td>
        </tr>
      </tbody>
    </table>
  </div>
</div><br>


<div class="row">
  <div class="col-xs-12">
    <h2>Schedule</h2>
  </div>

  TBD
</div>

<!-- 
<div class="row">
  <div class="col-xs-12">
     <table class="table table-striped">
      <tbody>
        <tr>
          <td>Welcome and Introduction</td>
          <td>8:45am - 9:00am</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 1</td>
          <td>9:00am - 9:25am</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 2</td>
          <td>9:25am - 9:50am</td>
        </tr>
        <tr>
          <td>Spotlight Talks (x3)</td>
          <td>9:50am - 10:10am</td>
        </tr>
        <tr>
          <td>Coffee Break and Poster Session</td>
          <td>10:10am - 11:10am</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 3</td>
          <td>11:10am - 11:35am</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 4</td>
          <td>11:35am - 12:00pm</td>
        </tr>
        <tr>
          <td>Lunch Break</td>
          <td>12:00pm - 1:30pm</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 5 (Industry Talks)</td>
          <td>1:30pm - 2:00pm</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 6</td>
          <td>2:00pm - 2:25pm</td>
        </tr>
        <tr>
          <td>Oral 1</td>
          <td>2:25pm - 2:45pm</td>
        </tr>
        <tr>
          <td>Oral 2</td>
          <td>2:45pm - 3:05pm</td>
        </tr>
        <tr>
          <td>Coffee Break and Poster Session</td>
          <td>3:05pm - 4:00pm</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 7</td>
          <td>4:00pm - 4:25pm</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 8</td>
          <td>4:25pm - 4:50pm</td>
        </tr>
        <tr>
          <td>Panel Discussion and Conclusion</td>
          <td>4:50pm - 5:40pm</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>
-->



<br>

## Accepted Papers 
<div class="row">
  <div class="col-md-12">
  TBD
  </div>
</div>

<br>



<!--
<div class="row">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>
-->

## Organizers 
<div class="row">

  <div class="col-xs-2">
    <a href="https://angelxuanchang.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/angel.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>
      <h6>Eloquent Labs, Simon Fraser University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://dritchie.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/daniel.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://dritchie.github.io/">Daniel Ritchie</a>
      <h6>Brown University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://www.cs.utexas.edu/~huangqx/">
      <img class="people-pic" src="{{ "/static/img/people/qixing.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cs.utexas.edu/~huangqx/">Qixing Huang</a>
      <h6>UT Austin</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="http://msavva.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/manolis.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://msavva.github.io/">Manolis Savva</a>
      <h6>Facebook AI Research, Simon Fraser University</h6>
    </div>
  </div>
</div>

<hr>

{% if page.acknowledgements %}
<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgments</h2>
  </div>
</div>
<a name="/acknowledgements"></a>
<div class="row">
  <div class="col-xs-12">
    <p>
      Thanks to <span style="color:#1a1aff;font-weight:400;"> <a href="https://visualdialog.org/">visualdialog.org</a></span> for the webpage format.
    </p>
  </div>
</div>
{% endif %}

<br>


<!-- 
<div class="row">
  <div class="col-xs-12">
    <h2>References</h2>
  </div>
</div>

{:.paper}
<span>[1] Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models</span>{:.papertitle}  
<span>D. Ritchie, K. Wang, and Y.a. Lin</span>{:.authors}  
<span>_CoRR_, vol. arXiv:1811.12463, 2018</span>{:.journal}  

{:.paper}
<span>[2] GRAINS: Generative Recursive Autoencoders for INdoor Scenes</span>{:.papertitle}  
<span>M. Li, A.G. Patil, K. Xu, S. Chaudhuri, O. Khan, A. Shamir, C. Tu, B. Chen, D. Cohen-Or, and H. Zhang</span>{:.authors}  
<span>_CoRR_, vol. arXiv:1807.09193, 2018</span>{:.journal}  

{:.paper}
<span>[3] Gibson env: real-world perception for embodied agents</span>{:.papertitle}  
<span>F. Xia, A. R. Zamir, Z.Y. He, A. Sax, J. Malik, and S. Savarese</span>{:.authors}  
<span>Computer Vision and Pattern Recognition (CVPR), 2018 IEEE Conference on, IEEE, 2018</span>{:.journal}  

{:.paper}
<span>[4] VirtualHome: Simulating Household Activities via Programs</span>{:.papertitle}  
<span>X. Puig, K. Ra, M. Boben, J. Li, T. Wang, S. Fidler, and A. Torralba</span>{:.authors}  
<span>CVPR, 2018</span>{:.journal}  

{:.paper}
<span>[5] Embodied Question Answering</span>{:.papertitle}  
<span>A. Das, S. Datta, G. Gkioxari, S. Lee, D. Parikh, and D. Batra</span>{:.authors}  
<span>CVPR, 2018</span>{:.journal}  

{:.paper}
<span>[6] ScanComplete: Large-Scale Scene Completion and Semantic Segmentation for 3D Scans</span>{:.papertitle}  
<span>A. Dai, D. Ritchie, M. Bokeloh, S. Reed, J. Sturm, and M. Nießner</span>{:.authors}  
<span>Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2018</span>{:.journal}  

{:.paper}
<span>[7] SeeThrough: Finding Objects in Heavily Occluded Indoor Scene Images</span>{:.papertitle}  
<span>N. Mitra, V. Kim, E. Yumer, M. Hueting, N. Carr, and P. Reddy</span>{:.authors}  
<span>2018 International Conference on 3D Vision (3DV), 2018</span>{:.journal}  

{:.paper}
<span>[8] Matterport3D: Learning from RGB-D Data in Indoor Environments</span>{:.papertitle}  
<span>A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner, M. Savva, S. Song, A. Zeng, and Y. Zhang</span>{:.authors}  
<span>_International Conference on 3D Vision (3DV)_, 2017</span>{:.journal}  

{:.paper}
<span>[9] Joint 2D-3D-semantic data for indoor scene understanding</span>{:.papertitle}  
<span>I. Armeni, S. Sax, A.R. Zamir, and S. Savarese</span>{:.authors}  
<span>_arXiv preprint arXiv:1702.01105_, 2017</span>{:.journal}  

{:.paper}
<span>[10] MINOS: Multimodal Indoor Simulator for Navigation in Complex Environments</span>{:.papertitle}  
<span>M. Savva, A.X. Chang, A. Dosovitskiy, T. Funkhouser, and V. Koltun</span>{:.authors}  
<span>_arXiv:1712.03931_, 2017</span>{:.journal}  

{:.paper}
<span>[11] AI2-THOR: An interactive 3D environment for visual AI</span>{:.papertitle}  
<span>E. Kolve, R. Mottaghi, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi</span>{:.authors}  
<span>_arXiv preprint arXiv:1712.05474_, 2017</span>{:.journal}  

{:.paper}
<span>[12] Physically-Based Rendering for Indoor Scene Understanding Using Convolutional Neural Networks</span>{:.papertitle}  
<span>Y. Zhang, S. Song, E. Yumer, M. Savva, J.Y. Lee, H. Jin, and T. Funkhouser</span>{:.authors}  
<span>_The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2017</span>{:.journal}  

{:.paper}
<span>[13] Semantic scene completion from a single depth image</span>{:.papertitle}  
<span>S. Song, F. Yu, A. Zeng, A.X. Chang, M. Savva, and T. Funkhouser</span>{:.authors}  
<span>Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2017</span>{:.journal}  

{:.paper}
<span>[14] ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes</span>{:.papertitle}  
<span>A. Dai, A.X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner</span>{:.authors}  
<span>Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017</span>{:.journal}  

{:.paper}
<span>[15]  CARLA: An Open Urban Driving Simulator</span>{:.papertitle}  
<span>A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun</span>{:.authors}  
<span>1--16, Proceedings of the 1st Annual Conference on Robot Learning, 2017</span>{:.journal}  

{:.paper}
<span>[16] SceneNN: A Scene Meshes Dataset with aNNotations</span>{:.papertitle}  
<span>B.S. Hua, Q.H. Pham, D.T. Nguyen, M.K. Tran, L.F. Yu, and S.K. Yeung</span>{:.authors}  
<span>International Conference on 3D Vision (3DV), 2016</span>{:.journal}  


-->